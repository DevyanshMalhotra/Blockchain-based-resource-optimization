
# Adaptive Resource Optimization in Blockchain using Reinforcement Learning

This project presents a novel approach for optimizing resource utilization in blockchain networks using **Deep Reinforcement Learning (DRL)**. It leverages intelligent, adaptive policy learning to improve network efficiency in terms of latency, throughput, and orphan block rate.

## ğŸ” Overview

Traditional blockchain networks suffer from inefficient use of bandwidth, high latency, and transaction congestion. This project introduces a Reinforcement Learning-based decision-making framework that:

- Monitors real-time local metrics (latency, mempool depth, bandwidth)
- Learns optimal peer selection and block assembly strategies
- Continuously improves through feedback from the environment

## âœ… Features

- âœ… Actor-Critic based DRL (DDPG and P-DQN)
- âœ… SimPy-based discrete-event blockchain simulator
- âœ… Prioritized Experience Replay Buffer
- âœ… Modular training pipeline with benchmarking
- âœ… Comparative analysis with static and rule-based policies
- âœ… Detailed metrics tracking and visualization

## ğŸ§  Algorithms Used

### 1. Deep Deterministic Policy Gradient (DDPG)
- Actor-Critic architecture for continuous control
- Actor: Outputs neighbor selection weights, block size, and interval
- Critic: Estimates Q-value for state-action pairs
- Uses soft target updates and exploration noise

### 2. Parameterized DQN (P-DQN)
- Handles mixed discrete + continuous action spaces
- Discrete head chooses sampling strategy
- Parameter networks output continuous sub-actions

### 3. Prioritized Experience Replay
- Samples transitions based on temporal-difference (TD) error
- Improves sample efficiency and learning stability

### 4. SimPy Discrete-Event Simulation
- Models block propagation, peer communication, and transaction arrival
- Custom environment with `reset()` and `step(action)` interface

## ğŸ—ï¸ Project Structure

```
adaptive_rl_blockchain/
â”‚
â”œâ”€â”€ agent.py              # DDPG agent definition
â”œâ”€â”€ sim_env.py            # SimPy-based blockchain network simulator
â”œâ”€â”€ replay_buffer.py      # Prioritized experience replay memory
â”œâ”€â”€ metrics.py            # Logging and plotting for metrics
â”œâ”€â”€ train.py              # Training script
â”œâ”€â”€ compare.py            # Policy evaluation and comparison
â”œâ”€â”€ config.yaml           # Environment and training configuration
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md             # Project documentation
```

## ğŸ“ˆ Metrics and Results

The following metrics are tracked during training and evaluation:
- Average Latency
- Average Throughput
- Orphan Block Rate

Generated visualizations:
- `metrics.png`: Learning curve plots (latency, throughput, orphan rate)
- `comparison_bar.png`: Policy comparison chart (Static, Rule-based, RL)

## ğŸ§ª Evaluation

Three policies are compared:
- **Static**: Always selects all peers with fixed block size and interval
- **Rule-based**: Selects peers based on latency and mempool heuristics
- **RL-Orch**: Learns optimal strategy through continuous environment feedback

## âš™ï¸ Requirements

- Python 3.8+
- PyTorch
- SimPy
- NumPy
- Matplotlib
- PyYAML
- tqdm

Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸš€ Run Training

```bash
python train.py
```

## ğŸ“Š Run Evaluation

```bash
python compare.py
```

## ğŸ“‚ Output

- `metrics.csv`: Logged metrics per episode
- `metrics.png`: Learning curves
- `comparison_bar.png`: Bar chart comparing policies

