{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd9ed60",
   "metadata": {},
   "source": [
    "\n",
    "# RL for Blockchain Resource Optimization — **Algorithm Blueprint**\n",
    "This notebook is a **clean, publication-ready DRC blueprint** that documents the exact algorithmic structure used:\n",
    "- **DDPG** (continuous actions)\n",
    "- **P‑DQN** (mixed discrete + continuous)\n",
    "- **Prioritized Experience Replay**\n",
    "- **SimPy discrete‑event environment** (`reset`, `step`)\n",
    "\n",
    "It includes **mathematical definitions, interfaces, and code scaffolds** so your client can understand and cite the implementation while core logic remains protected pending final handover.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ba4e9",
   "metadata": {},
   "source": [
    "\n",
    "## Pipeline at a Glance\n",
    "1. `SimEnv.reset()` → initial state vector \\(s_0\\)  \n",
    "2. Agent policy \\(\\pi_\\theta\\) selects action \\(a_t\\)  \n",
    "3. `SimEnv.step(a_t)` → \\((s_{t+1}, r_t, done, info)\\)  \n",
    "4. Store \\((s_t, a_t, r_t, s_{t+1}, done)\\) in **Prioritized Replay**  \n",
    "5. Sample minibatches, compute TD targets, update networks  \n",
    "6. Soft-update target networks \\(\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebddcd8",
   "metadata": {},
   "source": [
    "\n",
    "## Reward (MAP‑style)\n",
    "\\[\n",
    "r_t = -\\alpha \\cdot \\text{latency}_t \\;+\\; \\beta \\cdot \\text{throughput}_t \\;-\\; \\gamma \\cdot \\text{orphan\\_rate}_t\n",
    "\\]\n",
    "Typical ranges: \\(\\alpha\\in[0.2,2],\\ \\beta\\in[10^{-4},10^{-2}],\\ \\gamma\\in[1,20]\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246b51b",
   "metadata": {},
   "source": [
    "## SimPy Environment — Interface & Contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea08f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimEnv:\n",
    "    \"\"\"\n",
    "    Discrete-event blockchain simulator (SimPy-based in full implementation).\n",
    "\n",
    "    State vector s_t (example):\n",
    "        [latency_to_peers_mean, mempool_depth, available_bandwidth,\n",
    "         recent_orphan_rate, recent_throughput]\n",
    "\n",
    "    Action vector a_t (DDPG):\n",
    "        [neighbors_mask[0:K], block_size_norm, block_interval_norm]\n",
    "\n",
    "    Mixed action (P-DQN):\n",
    "        discrete: strategy_id in {0..K-1}\n",
    "        continuous: [block_size_norm, block_interval_norm]\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    reset() -> state\n",
    "        Initialize network, return s0.\n",
    "\n",
    "    step(action) -> (state_prime, reward, done, info)\n",
    "        Advance simulation using gossip + block params from 'action'.\n",
    "        Returns:\n",
    "            state_prime : next state vector\n",
    "            reward : scalar as per reward function\n",
    "            done   : episode termination flag\n",
    "            info   : dict for logging (latency, throughput, orphan_rate, etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # NOTE: Full SimPy processes (tx arrivals, mining, delays) are withheld.\n",
    "        # Hooks below define the public contract used by the agents.\n",
    "\n",
    "    def reset(self):\n",
    "        # TODO: initialize internal SimPy processes and metrics\n",
    "        raise NotImplementedError(\"Withheld in teaser. Provided on final handover.\")\n",
    "\n",
    "    def step(self, action):\n",
    "        # TODO: apply action, run events for Δt, compute metrics & reward\n",
    "        raise NotImplementedError(\"Withheld in teaser. Provided on final handover.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d9ec5",
   "metadata": {},
   "source": [
    "## Prioritized Experience Replay (PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16befec4",
   "metadata": {},
   "source": [
    "\n",
    "Priority for transition \\(i\\): \\(p_i = (|\\delta_i| + \\epsilon)^\\alpha\\) where \\(\\delta_i\\) is TD‑error.  \n",
    "Sampling prob: \\(P(i)=\\frac{p_i}{\\sum_j p_j}\\).  \n",
    "Importance‑sampling weight: \\(w_i = \\left(\\frac{1}{N}\\cdot\\frac{1}{P(i)}\\right)^\\beta\\), normalized by \\(\\frac{w_i}{\\max_j w_j}\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40737269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    Prioritized replay with proportional prioritization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    capacity : int\n",
    "    alpha : float   # priority exponent\n",
    "    beta0 : float   # initial IS exponent\n",
    "    beta_inc : float  # beta schedule per sampling step\n",
    "    eps : float     # small constant to avoid zero priority\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity=100_000, alpha=0.6, beta0=0.4, beta_inc=1e-4, eps=1e-6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta0\n",
    "        self.beta_inc = beta_inc\n",
    "        self.eps = eps\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "\n",
    "    def add(self, s, a, r, s2, d, td_error=None):\n",
    "        if self.size < self.capacity:\n",
    "            self.states.append(s); self.actions.append(a)\n",
    "            self.rewards.append(r); self.next_states.append(s2)\n",
    "            self.dones.append(d)\n",
    "        else:\n",
    "            self.states[self.ptr] = s; self.actions[self.ptr] = a\n",
    "            self.rewards[self.ptr] = r; self.next_states[self.ptr] = s2\n",
    "            self.dones[self.ptr] = d\n",
    "\n",
    "        p = (abs(td_error) + self.eps) if td_error is not None else 1.0\n",
    "        self.priorities[self.ptr] = p ** self.alpha\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        assert self.size > 0, \"Buffer empty\"\n",
    "\n",
    "        probs = self.priorities[:self.size]\n",
    "        probs = probs / probs.sum()\n",
    "\n",
    "        idx = np.random.choice(self.size, size=batch_size, p=probs, replace=False)\n",
    "\n",
    "        # Importance-sampling weights\n",
    "        self.beta = min(1.0, self.beta + self.beta_inc)\n",
    "        weights = (self.size * probs[idx]) ** (-self.beta)\n",
    "        weights = weights / weights.max()\n",
    "\n",
    "        batch = dict(\n",
    "            states=[self.states[i] for i in idx],\n",
    "            actions=[self.actions[i] for i in idx],\n",
    "            rewards=[self.rewards[i] for i in idx],\n",
    "            next_states=[self.next_states[i] for i in idx],\n",
    "            dones=[self.dones[i] for i in idx],\n",
    "            idx=idx,\n",
    "            weights=weights.astype(np.float32)\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "    def update_priorities(self, idx, td_errors):\n",
    "        for i, e in zip(idx, td_errors):\n",
    "            self.priorities[i] = (abs(e) + self.eps) ** self.alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0abe2ea",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eff798",
   "metadata": {},
   "source": [
    "\n",
    "Actor \\(\\mu_\\theta(s)\\) outputs continuous action \\(a\\).  \n",
    "Critic \\(Q_\\phi(s,a)\\) approximates state‑action value.  \n",
    "Targets: \\(y=r + \\gamma Q_{\\phi^-}(s', \\mu_{\\theta^-}(s'))\\).  \n",
    "Soft update: \\(\\theta^- \\leftarrow \\tau\\theta + (1-\\tau)\\theta^-\\), similarly for \\(\\phi^-\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class DDPGAgent:\n",
    "    \"\"\"\n",
    "    DDPG skeleton with exploration noise and soft target updates.\n",
    "\n",
    "    NOTE: Network architectures and optimizers are intentionally omitted.\n",
    "    Replace '... raise NotImplementedError' with your model code at handover.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, cfg):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.cfg = cfg\n",
    "        # self.actor = ...\n",
    "        # self.critic = ...\n",
    "        # self.actor_tgt = ...\n",
    "        # self.critic_tgt = ...\n",
    "        # self.opt_actor = ...\n",
    "        # self.opt_critic = ...\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.noise_sigma = cfg.get(\"noise_sigma\", 0.2)\n",
    "        self.noise_decay = cfg.get(\"noise_decay\", 0.999)\n",
    "        self.tau = cfg.get(\"tau\", 0.005)\n",
    "        self.gamma = cfg.get(\"gamma\", 0.99)\n",
    "\n",
    "    def select_action(self, state, explore=True):\n",
    "        # a = self.actor(state)\n",
    "        # placeholder: zero vector + noise\n",
    "        a = np.zeros(self.action_dim, dtype=np.float32)\n",
    "        if explore:\n",
    "            a = a + np.random.normal(0, self.noise_sigma, size=self.action_dim)\n",
    "        return a\n",
    "\n",
    "    def learn(self, batch):\n",
    "        \"\"\"\n",
    "        Use batch = {states, actions, rewards, next_states, dones, weights}\n",
    "        to update critic and actor.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Core update logic withheld for final delivery.\")\n",
    "\n",
    "    def soft_update(self, online, target):\n",
    "        # for online_param, target_param in zip(online.parameters(), target.parameters()):\n",
    "        #     target_param.data.copy_(self.tau * online_param.data + (1 - self.tau) * target_param.data)\n",
    "        raise NotImplementedError(\"Withheld.\")\n",
    "\n",
    "    def decay_exploration_noise(self):\n",
    "        self.noise_sigma *= self.noise_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5a446",
   "metadata": {},
   "source": [
    "## Parameterized DQN (P‑DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9831b1b",
   "metadata": {},
   "source": [
    "\n",
    "Two components:  \n",
    "1) Discrete Q‑head \\(Q(s,k)\\) for strategy \\(k\\in\\{0,\\dots,K-1\\}\\) (ε‑greedy for exploration).  \n",
    "2) Parameter networks output continuous \\(\\theta_k(s)\\) (e.g., block size, interval).  \n",
    "Action = \\((k, \\theta_k(s))\\).  \n",
    "Target: \\(y = r + \\gamma \\max_{k'} Q^-(s',k')\\) with double‑DQN option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4539fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class PDQNAgent:\n",
    "    \"\"\"\n",
    "    Parameterized DQN skeleton for mixed action spaces.\n",
    "\n",
    "    NOTE: Neural modules are deliberately abstracted.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, num_strategies, param_dim, cfg):\n",
    "        self.state_dim = state_dim\n",
    "        self.num_strategies = num_strategies  # K\n",
    "        self.param_dim = param_dim            # e.g., [block_size, block_interval]\n",
    "        self.cfg = cfg\n",
    "        self.eps = cfg.get(\"eps_start\", 1.0)\n",
    "        self.eps_min = cfg.get(\"eps_min\", 0.05)\n",
    "        self.eps_decay = cfg.get(\"eps_decay\", 0.995)\n",
    "        self.gamma = cfg.get(\"gamma\", 0.99)\n",
    "        # self.q_net = ...\n",
    "        # self.q_tgt = ...\n",
    "        # self.param_nets = [...]\n",
    "\n",
    "    def select_action(self, state, explore=True):\n",
    "        if explore and np.random.rand() < self.eps:\n",
    "            k = np.random.randint(self.num_strategies)\n",
    "        else:\n",
    "            # k = argmax_k Q(s,k)  # placeholder\n",
    "            k = 0\n",
    "        # theta = param_nets[k](s)  # placeholder\n",
    "        theta = np.zeros(self.param_dim, dtype=np.float32)\n",
    "        return (k, theta)\n",
    "\n",
    "    def learn(self, batch):\n",
    "        raise NotImplementedError(\"Core PDQN update logic withheld for final delivery.\")\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.eps = max(self.eps_min, self.eps * self.eps_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c155eb",
   "metadata": {},
   "source": [
    "## Training Loop — Contract (Pseudo-Executable Skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b625c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop(env, agent, replay, metrics, episodes=10, batch_size=64):\n",
    "    \"\"\"\n",
    "    Contract of the main loop (structure only).\n",
    "    \"\"\"\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = agent.select_action(s, explore=True)\n",
    "            s2, r, done, info = env.step(a)\n",
    "            replay.add(s, a, r, s2, float(done))\n",
    "\n",
    "            if replay.size >= batch_size:\n",
    "                batch = replay.sample(batch_size)\n",
    "                agent.learn(batch)\n",
    "\n",
    "            # metrics.record(r, info)  # implement externally\n",
    "            s = s2\n",
    "\n",
    "        # agent.soft_update(...); agent.decay_exploration_noise()\n",
    "        # if isinstance(agent, PDQNAgent): agent.decay_epsilon()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac055205",
   "metadata": {},
   "source": [
    "## Evaluation & Plots (Interfaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.history = []  # append dicts with {'reward':..., 'latency':..., 'throughput':..., 'orphan_rate':...}\n",
    "\n",
    "    def record(self, reward, info):\n",
    "        row = {'reward': reward}\n",
    "        row.update(info or {})\n",
    "        self.history.append(row)\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        import pandas as pd\n",
    "        return pd.DataFrame(self.history)\n",
    "\n",
    "    def plot_learning_curve(self, df=None):\n",
    "        import matplotlib.pyplot as plt\n",
    "        if df is None:\n",
    "            df = self.to_dataframe()\n",
    "        plt.figure()\n",
    "        plt.plot(df['reward'])\n",
    "        plt.title('Reward vs. step')\n",
    "        plt.xlabel('step'); plt.ylabel('reward')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_throughput_vs_latency(self, df=None):\n",
    "        import matplotlib.pyplot as plt\n",
    "        if df is None:\n",
    "            df = self.to_dataframe()\n",
    "        plt.figure()\n",
    "        plt.scatter(df['latency'], df['throughput'])\n",
    "        plt.title('Throughput vs. Confirmation Latency')\n",
    "        plt.xlabel('latency'); plt.ylabel('throughput')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_orphan_rate_over_time(self, df=None):\n",
    "        import matplotlib.pyplot as plt\n",
    "        if df is None:\n",
    "            df = self.to_dataframe()\n",
    "        plt.figure()\n",
    "        plt.plot(df['orphan_rate'])\n",
    "        plt.title('Orphan Block Rate Over Time')\n",
    "        plt.xlabel('step'); plt.ylabel('orphan_rate')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
